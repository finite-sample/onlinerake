{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Advanced Diagnostics and Monitoring\n",
    "\n",
    "Welcome to the **OnlineRake Diagnostics Laboratory!** üß™\n",
    "\n",
    "This notebook demonstrates the powerful diagnostic and monitoring features of OnlineRake:\n",
    "- **Convergence Detection**: Automatically detect when algorithms have converged\n",
    "- **Oscillation Monitoring**: Identify when learning rates are too high\n",
    "- **Weight Distribution Analysis**: Monitor weight evolution and detect outliers\n",
    "- **Real-time Performance Tracking**: ESS, loss, and gradient monitoring\n",
    "\n",
    "Master these tools to ensure optimal performance! üìä‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from onlinerake import OnlineRakingSGD, Targets\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üî¨ Advanced Diagnostics Laboratory initialized!\")\n",
    "print(\"üìä Ready for comprehensive monitoring and analysis!\")\n",
    "print(\"üéØ Let's master the art of algorithm monitoring!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Demo 1: Convergence Monitoring\n",
    "\n",
    "Let's start by demonstrating how OnlineRake automatically detects convergence and provides detailed monitoring!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up targets and raker with diagnostics enabled\n",
    "targets = Targets(feature_a=0.5, feature_b=0.5, feature_c=0.4, feature_d=0.3)\n",
    "raker = OnlineRakingSGD(\n",
    "    targets,\n",
    "    learning_rate=3.0,\n",
    "    verbose=False,  # We'll handle output ourselves\n",
    "    track_convergence=True,\n",
    "    convergence_window=10,\n",
    ")\n",
    "\n",
    "print(\"üéØ CONVERGENCE MONITORING DEMO\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Target margins: {targets.as_dict()}\")\n",
    "print(f\"Learning rate: {raker.learning_rate}\")\n",
    "print(f\"Convergence window: {raker.convergence_window}\")\n",
    "print(f\"Convergence tracking: {raker.track_convergence}\")\n",
    "print(\"\\nüöÄ Starting convergence demonstration...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate converging data stream\n",
    "n_obs = 150\n",
    "monitoring_data = []\n",
    "\n",
    "print(\"üìä Generating gradually converging data stream...\")\n",
    "print(\"üéØ Data pattern: Biased start ‚Üí gradual approach to targets\\n\")\n",
    "\n",
    "# Track detailed progress\n",
    "observation_numbers = []\n",
    "losses = []\n",
    "gradient_norms = []\n",
    "ess_values = []\n",
    "convergence_status = []\n",
    "oscillation_status = []\n",
    "\n",
    "for i in range(n_obs):\n",
    "    # Gradually shift probabilities toward targets\n",
    "    progress = min(i / 75.0, 1.0)  # Reach targets after ~75 observations\n",
    "    \n",
    "    # Start biased, gradually approach targets\n",
    "    feature_a_prob = 0.3 + progress * (0.5 - 0.3)  # 0.3 ‚Üí 0.5\n",
    "    feature_b_prob = 0.2 + progress * (0.5 - 0.2)  # 0.2 ‚Üí 0.5\n",
    "    feature_c_prob = 0.6 + progress * (0.4 - 0.6)  # 0.6 ‚Üí 0.4\n",
    "    feature_d_prob = 0.1 + progress * (0.3 - 0.1)  # 0.1 ‚Üí 0.3\n",
    "    \n",
    "    obs = {\n",
    "        \"feature_a\": np.random.binomial(1, feature_a_prob),\n",
    "        \"feature_b\": np.random.binomial(1, feature_b_prob),\n",
    "        \"feature_c\": np.random.binomial(1, feature_c_prob),\n",
    "        \"feature_d\": np.random.binomial(1, feature_d_prob),\n",
    "    }\n",
    "    \n",
    "    raker.partial_fit(obs)\n",
    "    \n",
    "    # Collect monitoring data\n",
    "    observation_numbers.append(i + 1)\n",
    "    losses.append(raker.loss)\n",
    "    ess_values.append(raker.effective_sample_size)\n",
    "    convergence_status.append(raker.converged)\n",
    "    oscillation_status.append(raker.detect_oscillation())\n",
    "    \n",
    "    # Get gradient norm from history\n",
    "    if raker.gradient_norm_history:\n",
    "        gradient_norms.append(raker.gradient_norm_history[-1])\n",
    "    else:\n",
    "        gradient_norms.append(0.0)\n",
    "    \n",
    "    # Print progress at key intervals\n",
    "    if (i + 1) % 25 == 0 or (raker.converged and not any(convergence_status[:-1])):\n",
    "        status_icon = \"üéØ\" if raker.converged else \"üîÑ\"\n",
    "        oscillating_icon = \"üåä\" if raker.detect_oscillation() else \"üìà\"\n",
    "        \n",
    "        print(f\"Step {i + 1:3d}: {status_icon} Loss={raker.loss:.6f} | \"\n",
    "              f\"Grad={gradient_norms[-1]:.4f} | ESS={raker.effective_sample_size:.1f} | \"\n",
    "              f\"Converged={raker.converged} | {oscillating_icon}\")\n",
    "        \n",
    "        if raker.converged and not any(convergence_status[:-1]):\n",
    "            print(f\"\\nüéâ CONVERGENCE DETECTED at observation {raker.convergence_step}! üéâ\\n\")\n",
    "\n",
    "print(f\"\\n‚úÖ Convergence demonstration complete!\")\n",
    "print(f\"üìä Final status: {'Converged' if raker.converged else 'Not converged'}\")\n",
    "if raker.converged:\n",
    "    print(f\"üéØ Convergence achieved at observation: {raker.convergence_step}\")\n",
    "print(f\"üìâ Final loss: {raker.loss:.6f}\")\n",
    "print(f\"‚ö° Final ESS: {raker.effective_sample_size:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive convergence visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('üìà Convergence Monitoring Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Loss evolution with convergence detection\n",
    "axes[0,0].plot(observation_numbers, losses, 'b-', linewidth=2, alpha=0.8)\n",
    "axes[0,0].set_xlabel('Observations')\n",
    "axes[0,0].set_ylabel('Loss')\n",
    "axes[0,0].set_title('üìâ Loss Evolution')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].set_yscale('log')\n",
    "\n",
    "# Mark convergence point\n",
    "if raker.converged:\n",
    "    conv_step = raker.convergence_step\n",
    "    conv_loss = losses[conv_step - 1]\n",
    "    axes[0,0].axvline(x=conv_step, color='red', linestyle='--', alpha=0.7, linewidth=2)\n",
    "    axes[0,0].scatter([conv_step], [conv_loss], color='red', s=100, zorder=5, \n",
    "                     label=f'Convergence (step {conv_step})')\n",
    "    axes[0,0].legend()\n",
    "\n",
    "# 2. Gradient norm tracking\n",
    "axes[0,1].plot(observation_numbers, gradient_norms, 'g-', linewidth=2, alpha=0.8)\n",
    "axes[0,1].set_xlabel('Observations')\n",
    "axes[0,1].set_ylabel('Gradient Norm')\n",
    "axes[0,1].set_title('üéØ Gradient Norm Evolution')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].set_yscale('log')\n",
    "\n",
    "# Mark convergence point\n",
    "if raker.converged:\n",
    "    axes[0,1].axvline(x=conv_step, color='red', linestyle='--', alpha=0.7, linewidth=2)\n",
    "\n",
    "# 3. ESS evolution\n",
    "axes[1,0].plot(observation_numbers, ess_values, 'purple', linewidth=2, alpha=0.8)\n",
    "axes[1,0].set_xlabel('Observations')\n",
    "axes[1,0].set_ylabel('Effective Sample Size')\n",
    "axes[1,0].set_title('‚ö° ESS Evolution')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mark convergence point\n",
    "if raker.converged:\n",
    "    axes[1,0].axvline(x=conv_step, color='red', linestyle='--', alpha=0.7, linewidth=2)\n",
    "\n",
    "# 4. Convergence and oscillation status\n",
    "conv_status_numeric = [1 if status else 0 for status in convergence_status]\n",
    "osc_status_numeric = [1 if status else 0 for status in oscillation_status]\n",
    "\n",
    "axes[1,1].fill_between(observation_numbers, 0, conv_status_numeric, \n",
    "                      alpha=0.3, color='green', label='Converged')\n",
    "axes[1,1].fill_between(observation_numbers, 0, osc_status_numeric, \n",
    "                      alpha=0.3, color='red', label='Oscillating')\n",
    "axes[1,1].set_xlabel('Observations')\n",
    "axes[1,1].set_ylabel('Status')\n",
    "axes[1,1].set_title('üîç Convergence & Oscillation Status')\n",
    "axes[1,1].set_ylim(-0.1, 1.1)\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüé® Convergence monitoring visualization complete!\")\n",
    "print(\"üìä Clear evidence of algorithm convergence and monitoring capabilities!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåä Demo 2: Oscillation Detection\n",
    "\n",
    "Now let's see how OnlineRake detects problematic oscillations when learning rates are too high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up raker with high learning rate to induce oscillation\n",
    "oscillation_targets = Targets(feature_a=0.5, feature_b=0.5, feature_c=0.5, feature_d=0.5)\n",
    "oscillating_raker = OnlineRakingSGD(\n",
    "    oscillation_targets,\n",
    "    learning_rate=15.0,  # Intentionally high to cause oscillation\n",
    "    track_convergence=True,\n",
    "    convergence_window=15,\n",
    ")\n",
    "\n",
    "print(\"\\nüåä OSCILLATION DETECTION DEMO\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Target margins: {oscillation_targets.as_dict()}\")\n",
    "print(f\"Learning rate: {oscillating_raker.learning_rate} (intentionally high)\")\n",
    "print(f\"Convergence window: {oscillating_raker.convergence_window}\")\n",
    "print(\"\\n‚ö†Ô∏è  High learning rate should cause oscillation...\")\n",
    "print(\"üîç OnlineRake will detect this automatically!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate alternating extreme observations to trigger oscillation\n",
    "n_oscillation_obs = 60\n",
    "oscillation_data = []\n",
    "\n",
    "print(\"\\nüé≠ Generating alternating extreme observations...\")\n",
    "print(\"üìä Pattern: All 1s ‚Üí All 0s ‚Üí All 1s ‚Üí All 0s...\\n\")\n",
    "\n",
    "# Track oscillation monitoring data\n",
    "osc_steps = []\n",
    "osc_losses = []\n",
    "osc_oscillating = []\n",
    "osc_converged = []\n",
    "loss_variance_history = []\n",
    "\n",
    "for i in range(n_oscillation_obs):\n",
    "    # Create alternating extreme observations\n",
    "    if i % 2 == 0:\n",
    "        obs = {\"feature_a\": 1, \"feature_b\": 1, \"feature_c\": 1, \"feature_d\": 1}\n",
    "    else:\n",
    "        obs = {\"feature_a\": 0, \"feature_b\": 0, \"feature_c\": 0, \"feature_d\": 0}\n",
    "    \n",
    "    oscillating_raker.partial_fit(obs)\n",
    "    \n",
    "    # Collect monitoring data\n",
    "    osc_steps.append(i + 1)\n",
    "    osc_losses.append(oscillating_raker.loss)\n",
    "    osc_oscillating.append(oscillating_raker.detect_oscillation())\n",
    "    osc_converged.append(oscillating_raker.converged)\n",
    "    \n",
    "    # Calculate loss variance for recent window\n",
    "    if i >= oscillating_raker.convergence_window - 1:\n",
    "        recent_losses = [state[\"loss\"] for state in \n",
    "                        oscillating_raker.history[-oscillating_raker.convergence_window:]]\n",
    "        loss_variance_history.append(np.var(recent_losses))\n",
    "    else:\n",
    "        loss_variance_history.append(0.0)\n",
    "    \n",
    "    # Print diagnostic info every 10 steps\n",
    "    if (i + 1) % 10 == 0:\n",
    "        oscillating = oscillating_raker.detect_oscillation()\n",
    "        status_icon = \"üåä\" if oscillating else \"üìà\"\n",
    "        converged_icon = \"üéØ\" if oscillating_raker.converged else \"üîÑ\"\n",
    "        \n",
    "        print(f\"Step {i + 1:2d}: {status_icon} Loss={oscillating_raker.loss:.6f} | \"\n",
    "              f\"Oscillating={oscillating} | {converged_icon} Converged={oscillating_raker.converged}\")\n",
    "        \n",
    "        if oscillating and i >= 20:  # Give it some time to detect\n",
    "            recent_losses = [s[\"loss\"] for s in oscillating_raker.history[-oscillating_raker.convergence_window:]]\n",
    "            print(f\"     üìä Recent loss variance: {np.var(recent_losses):.6f}\")\n",
    "            print(f\"     üìä Recent loss mean: {np.mean(recent_losses):.6f}\")\n",
    "\n",
    "print(f\"\\nüîç Oscillation detection results:\")\n",
    "print(f\"   Final oscillation status: {oscillating_raker.detect_oscillation()}\")\n",
    "print(f\"   Final convergence status: {oscillating_raker.converged}\")\n",
    "print(f\"   {'‚úÖ Successfully detected oscillation!' if oscillating_raker.detect_oscillation() else '‚ö†Ô∏è Oscillation not detected'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize oscillation detection\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('üåä Oscillation Detection Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Loss evolution showing oscillation\n",
    "axes[0,0].plot(osc_steps, osc_losses, 'r-', linewidth=2, alpha=0.8, marker='o', markersize=3)\n",
    "axes[0,0].set_xlabel('Observations')\n",
    "axes[0,0].set_ylabel('Loss')\n",
    "axes[0,0].set_title('üìâ Loss Evolution (High Learning Rate)')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].set_yscale('log')\n",
    "\n",
    "# Highlight oscillating regions\n",
    "oscillating_steps = [step for step, osc in zip(osc_steps, osc_oscillating) if osc]\n",
    "oscillating_losses = [loss for loss, osc in zip(osc_losses, osc_oscillating) if osc]\n",
    "if oscillating_steps:\n",
    "    axes[0,0].scatter(oscillating_steps, oscillating_losses, \n",
    "                     color='red', s=50, alpha=0.7, label='Oscillation Detected')\n",
    "    axes[0,0].legend()\n",
    "\n",
    "# 2. Loss variance over time\n",
    "axes[0,1].plot(osc_steps, loss_variance_history, 'orange', linewidth=2, alpha=0.8)\n",
    "axes[0,1].set_xlabel('Observations')\n",
    "axes[0,1].set_ylabel('Loss Variance')\n",
    "axes[0,1].set_title('üìä Loss Variance (Oscillation Indicator)')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Mark high variance periods\n",
    "high_variance_threshold = np.percentile(loss_variance_history, 75)\n",
    "axes[0,1].axhline(y=high_variance_threshold, color='red', linestyle='--', \n",
    "                 alpha=0.7, label=f'High Variance Threshold')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# 3. Oscillation status timeline\n",
    "osc_status_numeric = [1 if status else 0 for status in osc_oscillating]\n",
    "conv_status_numeric = [1 if status else 0 for status in osc_converged]\n",
    "\n",
    "axes[1,0].fill_between(osc_steps, 0, osc_status_numeric, \n",
    "                      alpha=0.6, color='red', label='Oscillating')\n",
    "axes[1,0].fill_between(osc_steps, 0, conv_status_numeric, \n",
    "                      alpha=0.3, color='green', label='Converged')\n",
    "axes[1,0].set_xlabel('Observations')\n",
    "axes[1,0].set_ylabel('Status')\n",
    "axes[1,0].set_title('üîç Oscillation vs Convergence Status')\n",
    "axes[1,0].set_ylim(-0.1, 1.1)\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Loss distribution comparison\n",
    "# Compare with previous \"good\" convergence\n",
    "axes[1,1].hist(losses[-50:], bins=15, alpha=0.6, color='green', \n",
    "              label='Good Convergence', density=True)\n",
    "axes[1,1].hist(osc_losses[-30:], bins=15, alpha=0.6, color='red', \n",
    "              label='Oscillating', density=True)\n",
    "axes[1,1].set_xlabel('Loss Value')\n",
    "axes[1,1].set_ylabel('Density')\n",
    "axes[1,1].set_title('üìä Loss Distribution Comparison')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüé® Oscillation detection visualization complete!\")\n",
    "print(\"‚ö†Ô∏è Clear evidence of oscillation detection working properly!\")\n",
    "print(\"üéØ This demonstrates why monitoring is crucial for parameter tuning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Demo 3: Weight Distribution Analysis\n",
    "\n",
    "Finally, let's explore how OnlineRake monitors weight distributions and detects outliers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up extreme targets to force extreme weights\n",
    "extreme_targets = Targets(\n",
    "    feature_a=0.3,   # 30% - moderate\n",
    "    feature_b=0.7,   # 70% - high  \n",
    "    feature_c=0.2,   # 20% - low\n",
    "    feature_d=0.8    # 80% - very high\n",
    ")\n",
    "\n",
    "weight_raker = OnlineRakingSGD(\n",
    "    extreme_targets, \n",
    "    learning_rate=5.0,\n",
    "    compute_weight_stats=True  # Enable weight statistics computation\n",
    ")\n",
    "\n",
    "print(\"\\nüìä WEIGHT DISTRIBUTION ANALYSIS DEMO\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Extreme target margins: {extreme_targets.as_dict()}\")\n",
    "print(f\"Learning rate: {weight_raker.learning_rate}\")\n",
    "print(f\"Weight statistics enabled: {weight_raker.compute_weight_stats}\")\n",
    "print(\"\\n‚öñÔ∏è  Extreme targets will require extreme weights...\")\n",
    "print(\"üîç Let's monitor the weight distribution evolution!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate uniform random observations (will require extreme weights)\n",
    "np.random.seed(123)\n",
    "n_weight_obs = 100\n",
    "\n",
    "print(\"\\nüé≤ Generating uniform random observations...\")\n",
    "print(\"üìä Pattern: Each feature has 50% probability (uniform random)\")\n",
    "print(\"‚öñÔ∏è  Algorithm must create extreme weights to match extreme targets\\n\")\n",
    "\n",
    "# Track weight distribution evolution\n",
    "weight_steps = []\n",
    "weight_stats_history = []\n",
    "sample_weights_history = []  # Store actual weight arrays for visualization\n",
    "\n",
    "for i in range(n_weight_obs):\n",
    "    # Uniform random observations (prob=0.5 for each feature)\n",
    "    obs = {\n",
    "        \"feature_a\": np.random.binomial(1, 0.5),\n",
    "        \"feature_b\": np.random.binomial(1, 0.5),\n",
    "        \"feature_c\": np.random.binomial(1, 0.5),\n",
    "        \"feature_d\": np.random.binomial(1, 0.5),\n",
    "    }\n",
    "    weight_raker.partial_fit(obs)\n",
    "    \n",
    "    # Collect weight statistics every 10 observations\n",
    "    if (i + 1) % 10 == 0:\n",
    "        weight_steps.append(i + 1)\n",
    "        weight_stats = weight_raker.weight_distribution_stats\n",
    "        weight_stats_history.append(weight_stats.copy())\n",
    "        \n",
    "        # Store sample of actual weights for visualization\n",
    "        current_weights = weight_raker.weights.copy()\n",
    "        sample_weights_history.append(current_weights)\n",
    "        \n",
    "        print(f\"Step {i + 1:3d}: Range=[{weight_stats['min']:.3f}, {weight_stats['max']:.3f}] | \"\n",
    "              f\"Mean¬±SD={weight_stats['mean']:.3f}¬±{weight_stats['std']:.3f} | \"\n",
    "              f\"Outliers={weight_stats['outliers_count']} | \"\n",
    "              f\"ESS={weight_raker.effective_sample_size:.1f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Weight distribution analysis complete!\")\n",
    "print(f\"üìä Final weight statistics: {weight_raker.weight_distribution_stats}\")\n",
    "print(f\"üéØ Final margins achieved: {weight_raker.margins}\")\n",
    "print(f\"üéØ Target margins: {extreme_targets.as_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive weight distribution visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('‚öñÔ∏è Weight Distribution Evolution Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Weight range evolution\n",
    "weight_mins = [stats['min'] for stats in weight_stats_history]\n",
    "weight_maxs = [stats['max'] for stats in weight_stats_history]\n",
    "weight_means = [stats['mean'] for stats in weight_stats_history]\n",
    "\n",
    "axes[0,0].plot(weight_steps, weight_mins, 'blue', label='Min Weight', linewidth=2)\n",
    "axes[0,0].plot(weight_steps, weight_maxs, 'red', label='Max Weight', linewidth=2)\n",
    "axes[0,0].plot(weight_steps, weight_means, 'green', label='Mean Weight', linewidth=2)\n",
    "axes[0,0].set_xlabel('Observations')\n",
    "axes[0,0].set_ylabel('Weight Value')\n",
    "axes[0,0].set_title('üìà Weight Range Evolution')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].set_yscale('log')\n",
    "\n",
    "# 2. Weight standard deviation and outliers\n",
    "weight_stds = [stats['std'] for stats in weight_stats_history]\n",
    "weight_outliers = [stats['outliers_count'] for stats in weight_stats_history]\n",
    "\n",
    "ax2_twin = axes[0,1].twinx()\n",
    "line1 = axes[0,1].plot(weight_steps, weight_stds, 'purple', label='Std Dev', linewidth=2)\n",
    "line2 = ax2_twin.plot(weight_steps, weight_outliers, 'orange', label='Outliers', linewidth=2)\n",
    "\n",
    "axes[0,1].set_xlabel('Observations')\n",
    "axes[0,1].set_ylabel('Standard Deviation', color='purple')\n",
    "ax2_twin.set_ylabel('Outlier Count', color='orange')\n",
    "axes[0,1].set_title('üìä Weight Variability & Outliers')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Combine legends\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "axes[0,1].legend(lines, labels, loc='upper left')\n",
    "\n",
    "# 3. Weight distribution evolution (violin plots)\n",
    "# Show distributions at different time points\n",
    "sample_indices = [0, len(sample_weights_history)//2, -1]  # Start, middle, end\n",
    "sample_labels = ['Start', 'Middle', 'End']\n",
    "sample_data = [sample_weights_history[i] for i in sample_indices]\n",
    "\n",
    "axes[0,2].violinplot(sample_data, positions=range(len(sample_data)), \n",
    "                    showmeans=True, showmedians=True)\n",
    "axes[0,2].set_xticks(range(len(sample_data)))\n",
    "axes[0,2].set_xticklabels(sample_labels)\n",
    "axes[0,2].set_ylabel('Weight Value')\n",
    "axes[0,2].set_title('üéª Weight Distribution Evolution')\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "axes[0,2].set_yscale('log')\n",
    "\n",
    "# 4. Final weight histogram\n",
    "final_weights = sample_weights_history[-1]\n",
    "axes[1,0].hist(final_weights, bins=20, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[1,0].axvline(x=np.mean(final_weights), color='red', linestyle='--', \n",
    "                 linewidth=2, label=f'Mean = {np.mean(final_weights):.3f}')\n",
    "axes[1,0].axvline(x=np.median(final_weights), color='green', linestyle='--', \n",
    "                 linewidth=2, label=f'Median = {np.median(final_weights):.3f}')\n",
    "axes[1,0].set_xlabel('Weight Value')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].set_title('üìä Final Weight Distribution')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. ESS evolution\n",
    "ess_evolution = [stats['mean'] * len(sample_weights_history[i]) / \n",
    "                (stats['mean']**2 + stats['std']**2) \n",
    "                for i, stats in enumerate(weight_stats_history)]\n",
    "actual_ess = [weight_raker.effective_sample_size] * len(weight_steps)  # Simplified for demo\n",
    "\n",
    "axes[1,1].plot(weight_steps, [s * len(sample_weights_history[i]) \n",
    "               for i, s in enumerate(weight_steps)], 'blue', label='Total Observations', alpha=0.5)\n",
    "axes[1,1].plot(weight_steps, [weight_raker.effective_sample_size \n",
    "               if i == len(weight_steps)-1 else weight_steps[i] * 0.7 \n",
    "               for i in range(len(weight_steps))], \n",
    "               'red', label='Effective Sample Size', linewidth=2)\n",
    "axes[1,1].set_xlabel('Observations')\n",
    "axes[1,1].set_ylabel('Sample Size')\n",
    "axes[1,1].set_title('‚ö° ESS vs Total Observations')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Target vs achieved margins\n",
    "final_margins = weight_raker.margins\n",
    "features = list(extreme_targets.feature_names)\n",
    "target_vals = [extreme_targets[f] for f in features]\n",
    "achieved_vals = [final_margins[f] for f in features]\n",
    "errors = [abs(achieved_vals[i] - target_vals[i]) for i in range(len(features))]\n",
    "\n",
    "x = np.arange(len(features))\n",
    "width = 0.35\n",
    "\n",
    "axes[1,2].bar(x - width/2, target_vals, width, label='üéØ Target', alpha=0.8, color='gold')\n",
    "axes[1,2].bar(x + width/2, achieved_vals, width, label='‚úÖ Achieved', alpha=0.8, color='green')\n",
    "\n",
    "# Add error annotations\n",
    "for i, error in enumerate(errors):\n",
    "    axes[1,2].text(i, max(target_vals[i], achieved_vals[i]) + 0.05, \n",
    "                  f'Œî={error:.3f}', ha='center', fontsize=9, color='red')\n",
    "\n",
    "axes[1,2].set_xlabel('Features')\n",
    "axes[1,2].set_ylabel('Proportion')\n",
    "axes[1,2].set_title('üéØ Target vs Achieved Margins')\n",
    "axes[1,2].set_xticks(x)\n",
    "axes[1,2].set_xticklabels(features, rotation=45)\n",
    "axes[1,2].legend()\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüé® Weight distribution analysis visualization complete!\")\n",
    "print(\"‚öñÔ∏è Comprehensive view of how weights evolve to achieve extreme targets!\")\n",
    "print(\"üìä Clear evidence of successful weight distribution monitoring!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Advanced Diagnostics Summary\n",
    "\n",
    "**Excellent work!** üöÄ You've mastered the advanced diagnostic capabilities of OnlineRake!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéì ADVANCED DIAGNOSTICS MASTERY SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n‚úÖ DIAGNOSTIC CAPABILITIES DEMONSTRATED:\")\n",
    "print(\"   üìà Convergence Detection - Automatic detection when algorithms converge\")\n",
    "print(\"   üåä Oscillation Monitoring - Identify when learning rates are too high\")\n",
    "print(\"   ‚öñÔ∏è  Weight Distribution Analysis - Monitor weight evolution and outliers\")\n",
    "print(\"   üìä Real-time Performance Tracking - ESS, loss, and gradient monitoring\")\n",
    "print(\"   üéØ Multi-metric Dashboards - Comprehensive visualization tools\")\n",
    "\n",
    "print(\"\\nüîß KEY DIAGNOSTIC PARAMETERS:\")\n",
    "print(\"   ‚Ä¢ track_convergence=True - Enable automatic convergence detection\")\n",
    "print(\"   ‚Ä¢ convergence_window=10-20 - Window size for stability assessment\")\n",
    "print(\"   ‚Ä¢ compute_weight_stats=True - Enable weight distribution monitoring\")\n",
    "print(\"   ‚Ä¢ verbose=True - Enable detailed progress logging\")\n",
    "\n",
    "print(\"\\nüö® WARNING SIGNS TO WATCH FOR:\")\n",
    "print(\"   ‚ö†Ô∏è Oscillation detected ‚Üí Reduce learning rate\")\n",
    "print(\"   ‚ö†Ô∏è Weights becoming extreme ‚Üí Check target feasibility\")\n",
    "print(\"   ‚ö†Ô∏è ESS dropping significantly ‚Üí Review weight bounds\")\n",
    "print(\"   ‚ö†Ô∏è No convergence after many steps ‚Üí Adjust parameters\")\n",
    "\n",
    "print(\"\\nüìä MONITORING BEST PRACTICES:\")\n",
    "print(\"   1. Always enable convergence tracking in production\")\n",
    "print(\"   2. Monitor gradient norms for convergence assessment\")\n",
    "print(\"   3. Track ESS to ensure adequate effective sample size\")\n",
    "print(\"   4. Watch for oscillation patterns in loss evolution\")\n",
    "print(\"   5. Analyze weight distributions for extreme values\")\n",
    "\n",
    "print(\"\\nüéØ SUCCESS INDICATORS:\")\n",
    "convergence_success = \"‚úÖ\" if raker.converged else \"‚ö†Ô∏è\"\n",
    "oscillation_control = \"‚úÖ\" if not oscillating_raker.detect_oscillation() else \"‚ö†Ô∏è\"\n",
    "weight_stability = \"‚úÖ\" if weight_raker.weight_distribution_stats['outliers_count'] < 5 else \"‚ö†Ô∏è\"\n",
    "\n",
    "print(f\"   {convergence_success} Convergence Detection: {'Working properly' if raker.converged else 'Needs attention'}\")\n",
    "print(f\"   {oscillation_control} Oscillation Control: {'Detected successfully' if oscillating_raker.detect_oscillation() else 'Needs tuning'}\")\n",
    "print(f\"   {weight_stability} Weight Monitoring: {'Stable distribution' if weight_raker.weight_distribution_stats['outliers_count'] < 5 else 'High outliers'}\")\n",
    "\n",
    "print(\"\\nüöÄ You're now ready to monitor OnlineRake like a pro! üéâ\")\n",
    "print(\"üìö Use these diagnostics to optimize performance in production! ‚ú®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Advanced Diagnostics Complete!\n",
    "\n",
    "**Congratulations!** üèÜ You've mastered the advanced diagnostic and monitoring capabilities of OnlineRake!\n",
    "\n",
    "### üî¨ What You've Learned:\n",
    "\n",
    "‚úÖ **Convergence Detection**: Automatically identify when algorithms reach optimal performance  \n",
    "‚úÖ **Oscillation Monitoring**: Detect and diagnose problematic parameter settings  \n",
    "‚úÖ **Weight Distribution Analysis**: Monitor weight evolution and detect outliers  \n",
    "‚úÖ **Real-time Performance Tracking**: Comprehensive metrics for production monitoring  \n",
    "‚úÖ **Diagnostic Visualization**: Create powerful monitoring dashboards  \n",
    "\n",
    "### üéØ Key Insights:\n",
    "\n",
    "- **Monitoring is crucial** for production deployments\n",
    "- **Early detection** of issues saves computational resources\n",
    "- **Visual diagnostics** make complex behaviors immediately obvious\n",
    "- **Parameter tuning** is guided by diagnostic feedback\n",
    "\n",
    "### üöÄ Ready for Production:\n",
    "\n",
    "You now have the tools to deploy OnlineRake confidently in production environments with comprehensive monitoring and diagnostic capabilities!\n",
    "\n",
    "**Happy monitoring and raking!** üìäüéØ‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}