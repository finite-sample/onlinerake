{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Getting Started with OnlineRake\n",
    "\n",
    "Welcome to **OnlineRake** - a powerful Python package for streaming survey weight calibration!\n",
    "\n",
    "This notebook demonstrates how to use OnlineRake to correct bias in real-time data streams using two state-of-the-art algorithms:\n",
    "- **SGD Raking**: Fast and effective for most scenarios\n",
    "- **MWU Raking**: Maintains positive weights through multiplicative updates\n",
    "\n",
    "Let's see these algorithms in action! ğŸ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from onlinerake import OnlineRakingSGD, OnlineRakingMWU, Targets\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ğŸ“¦ Libraries imported successfully!\")\n",
    "print(\"ğŸ¨ Plotting style configured\")\n",
    "print(\"ğŸ² Random seed set for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Example 1: Correcting Feature Bias in Online Survey\n",
    "\n",
    "Imagine you're running an online survey, but your responses are biased - certain features are under or over-represented compared to the target population. \n",
    "\n",
    "**OnlineRake to the rescue!** ğŸ¦¸â€â™‚ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our target population proportions\n",
    "targets = Targets(\n",
    "    feature_a=0.52,  # 52% have feature A\n",
    "    feature_b=0.51,  # 51% have feature B  \n",
    "    feature_c=0.35,  # 35% have feature C\n",
    "    feature_d=0.19,  # 19% have feature D\n",
    ")\n",
    "\n",
    "print(\"ğŸ¯ Target margins:\")\n",
    "for feature, target in targets.as_dict().items():\n",
    "    print(f\"   {feature}: {target:.1%}\")\n",
    "\n",
    "# Initialize both raking algorithms\n",
    "sgd_raker = OnlineRakingSGD(targets, learning_rate=4.0)\n",
    "mwu_raker = OnlineRakingMWU(targets, learning_rate=1.2)\n",
    "\n",
    "print(\"\\nğŸ”§ Rakers initialized!\")\n",
    "print(f\"   SGD learning rate: {sgd_raker.learning_rate}\")\n",
    "print(f\"   MWU learning rate: {mwu_raker.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate biased survey responses\n",
    "n_responses = 500\n",
    "raw_totals = {\"feature_a\": 0, \"feature_b\": 0, \"feature_c\": 0, \"feature_d\": 0}\n",
    "\n",
    "print(f\"ğŸ­ Simulating {n_responses} biased survey responses...\")\n",
    "print(\"ğŸ“‰ Bias pattern: Survey with feature underrepresentation\\n\")\n",
    "\n",
    "# Store history for plotting\n",
    "sgd_history = []\n",
    "mwu_history = []\n",
    "observation_numbers = []\n",
    "\n",
    "for i in range(n_responses):\n",
    "    # Generate biased observations\n",
    "    feature_a = 1 if np.random.random() < 0.30 else 0  # 30% vs target 52%\n",
    "    feature_b = 1 if np.random.random() < 0.35 else 0  # 35% vs target 51%\n",
    "    feature_c = 1 if np.random.random() < 0.60 else 0  # 60% vs target 35% \n",
    "    feature_d = 1 if np.random.random() < 0.15 else 0  # 15% vs target 19%\n",
    "    \n",
    "    obs = {\n",
    "        \"feature_a\": feature_a, \"feature_b\": feature_b, \n",
    "        \"feature_c\": feature_c, \"feature_d\": feature_d\n",
    "    }\n",
    "    \n",
    "    # Update both rakers\n",
    "    sgd_raker.partial_fit(obs)\n",
    "    mwu_raker.partial_fit(obs)\n",
    "    \n",
    "    # Track raw proportions\n",
    "    for key in raw_totals:\n",
    "        raw_totals[key] += obs[key]\n",
    "    \n",
    "    # Store history for plotting (every 25 observations)\n",
    "    if (i + 1) % 25 == 0:\n",
    "        observation_numbers.append(i + 1)\n",
    "        sgd_history.append(sgd_raker.margins.copy())\n",
    "        mwu_history.append(mwu_raker.margins.copy())\n",
    "\n",
    "print(\"âœ… Simulation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final results\n",
    "raw_margins = {k: v / n_responses for k, v in raw_totals.items()}\n",
    "sgd_margins = sgd_raker.margins\n",
    "mwu_margins = mwu_raker.margins\n",
    "\n",
    "print(\"ğŸ“‹ RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Feature':<12} {'Target':<8} {'Raw':<8} {'SGD':<8} {'MWU':<8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for feature in [\"feature_a\", \"feature_b\", \"feature_c\", \"feature_d\"]:\n",
    "    target = targets.as_dict()[feature]\n",
    "    raw = raw_margins[feature]\n",
    "    sgd = sgd_margins[feature]\n",
    "    mwu = mwu_margins[feature]\n",
    "    print(f\"{feature:<12} {target:<8.3f} {raw:<8.3f} {sgd:<8.3f} {mwu:<8.3f}\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ ALGORITHM PERFORMANCE\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Effective Sample Size:\")\n",
    "print(f\"   SGD: {sgd_raker.effective_sample_size:.1f}\")\n",
    "print(f\"   MWU: {mwu_raker.effective_sample_size:.1f}\")\n",
    "\n",
    "print(f\"\\nFinal Loss (squared error):\")\n",
    "print(f\"   SGD: {sgd_raker.loss:.6f}\")\n",
    "print(f\"   MWU: {mwu_raker.loss:.6f}\")\n",
    "\n",
    "if sgd_raker.loss < mwu_raker.loss:\n",
    "    print(\"\\nğŸ† SGD achieved lower loss!\")\n",
    "else:\n",
    "    print(\"\\nğŸ† MWU achieved lower loss!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('ğŸ¯ OnlineRake Results: Before vs After Calibration', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Before/After comparison\n",
    "features = list(targets.feature_names)\n",
    "target_vals = [targets[f] for f in features]\n",
    "raw_vals = [raw_margins[f] for f in features]\n",
    "sgd_vals = [sgd_margins[f] for f in features]\n",
    "mwu_vals = [mwu_margins[f] for f in features]\n",
    "\n",
    "x = np.arange(len(features))\n",
    "width = 0.2\n",
    "\n",
    "axes[0,0].bar(x - 1.5*width, target_vals, width, label='ğŸ¯ Target', alpha=0.8, color='gold')\n",
    "axes[0,0].bar(x - 0.5*width, raw_vals, width, label='âŒ Raw (Biased)', alpha=0.8, color='red')\n",
    "axes[0,0].bar(x + 0.5*width, sgd_vals, width, label='âœ… SGD Corrected', alpha=0.8, color='green')\n",
    "axes[0,0].bar(x + 1.5*width, mwu_vals, width, label='âœ… MWU Corrected', alpha=0.8, color='blue')\n",
    "\n",
    "axes[0,0].set_xlabel('Features')\n",
    "axes[0,0].set_ylabel('Proportion')\n",
    "axes[0,0].set_title('ğŸ“Š Target vs Raw vs Corrected Proportions')\n",
    "axes[0,0].set_xticks(x)\n",
    "axes[0,0].set_xticklabels(features, rotation=45)\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Convergence over time\n",
    "for i, feature in enumerate(features):\n",
    "    sgd_feature_history = [margins[feature] for margins in sgd_history]\n",
    "    mwu_feature_history = [margins[feature] for margins in mwu_history]\n",
    "    \n",
    "    axes[0,1].plot(observation_numbers, sgd_feature_history, '-', \n",
    "                  label=f'SGD {feature}' if i < 2 else '', alpha=0.7)\n",
    "    axes[0,1].plot(observation_numbers, mwu_feature_history, '--', \n",
    "                  label=f'MWU {feature}' if i < 2 else '', alpha=0.7)\n",
    "    \n",
    "    # Add target line\n",
    "    axes[0,1].axhline(y=targets[feature], color='red', linestyle=':', alpha=0.5)\n",
    "\n",
    "axes[0,1].set_xlabel('Observations')\n",
    "axes[0,1].set_ylabel('Margin')\n",
    "axes[0,1].set_title('ğŸ“ˆ Margin Convergence Over Time')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Error comparison\n",
    "sgd_errors = [abs(sgd_margins[f] - targets[f]) for f in features]\n",
    "mwu_errors = [abs(mwu_margins[f] - targets[f]) for f in features]\n",
    "raw_errors = [abs(raw_margins[f] - targets[f]) for f in features]\n",
    "\n",
    "x = np.arange(len(features))\n",
    "axes[1,0].bar(x - width, raw_errors, width, label='âŒ Raw Error', alpha=0.8, color='red')\n",
    "axes[1,0].bar(x, sgd_errors, width, label='âœ… SGD Error', alpha=0.8, color='green')\n",
    "axes[1,0].bar(x + width, mwu_errors, width, label='âœ… MWU Error', alpha=0.8, color='blue')\n",
    "\n",
    "axes[1,0].set_xlabel('Features')\n",
    "axes[1,0].set_ylabel('Absolute Error')\n",
    "axes[1,0].set_title('ğŸ“‰ Error Reduction by Feature')\n",
    "axes[1,0].set_xticks(x)\n",
    "axes[1,0].set_xticklabels(features, rotation=45)\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Performance metrics\n",
    "metrics = ['Loss', 'ESS']\n",
    "sgd_metrics = [sgd_raker.loss, sgd_raker.effective_sample_size]\n",
    "mwu_metrics = [mwu_raker.loss, mwu_raker.effective_sample_size]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "axes[1,1].bar(x - width/2, sgd_metrics, width, label='SGD', alpha=0.8, color='green')\n",
    "axes[1,1].bar(x + width/2, mwu_metrics, width, label='MWU', alpha=0.8, color='blue')\n",
    "\n",
    "axes[1,1].set_xlabel('Metrics')\n",
    "axes[1,1].set_ylabel('Value')\n",
    "axes[1,1].set_title('âš¡ Algorithm Performance Comparison')\n",
    "axes[1,1].set_xticks(x)\n",
    "axes[1,1].set_xticklabels(metrics)\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ¨ Visualization complete! Clear evidence that OnlineRake works! âœ¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒŠ Example 2: Real-time Feature Tracking with Pattern Shifts\n",
    "\n",
    "Now let's see how OnlineRake handles **changing data patterns over time** - a common challenge in real-world streaming data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up new scenario with different targets\n",
    "streaming_targets = Targets(\n",
    "    feature_a=0.48,  # 48% have feature A\n",
    "    feature_b=0.53,  # 53% have feature B\n",
    "    feature_c=0.32,  # 32% have feature C\n",
    "    feature_d=0.17,  # 17% have feature D\n",
    ")\n",
    "\n",
    "print(\"ğŸŒŠ STREAMING SCENARIO: Time-varying bias patterns\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ¯ Target feature margins:\")\n",
    "for feature, target in streaming_targets.as_dict().items():\n",
    "    print(f\"   {feature}: {target:.1%}\")\n",
    "\n",
    "raker = OnlineRakingSGD(streaming_targets, learning_rate=3.0)\n",
    "print(f\"\\nğŸš€ SGD Raker initialized with learning rate: {raker.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data with time-varying bias\n",
    "np.random.seed(789)\n",
    "n_obs = 1000\n",
    "\n",
    "print(f\"\\nğŸ­ Simulating {n_obs} observations with time-varying bias...\")\n",
    "print(\"ğŸ“Š Pattern: Feature probabilities change over time\\n\")\n",
    "\n",
    "# Track evolution\n",
    "margin_history = []\n",
    "loss_history = []\n",
    "ess_history = []\n",
    "time_points = []\n",
    "\n",
    "for i in range(n_obs):\n",
    "    # Feature patterns change over time\n",
    "    time_factor = i / n_obs\n",
    "    \n",
    "    # Feature A: increases over time (0.2 â†’ 0.6)\n",
    "    p_feature_a = 0.2 + 0.4 * time_factor\n",
    "    feature_a = 1 if np.random.random() < p_feature_a else 0\n",
    "    \n",
    "    # Feature B: relatively stable\n",
    "    feature_b = 1 if np.random.random() < 0.52 else 0\n",
    "    \n",
    "    # Feature C: decreases over time (0.6 â†’ 0.3)\n",
    "    p_feature_c = 0.6 - 0.3 * time_factor\n",
    "    feature_c = 1 if np.random.random() < p_feature_c else 0\n",
    "    \n",
    "    # Feature D: relatively stable\n",
    "    feature_d = 1 if np.random.random() < 0.18 else 0\n",
    "    \n",
    "    obs = {\n",
    "        \"feature_a\": feature_a, \"feature_b\": feature_b,\n",
    "        \"feature_c\": feature_c, \"feature_d\": feature_d\n",
    "    }\n",
    "    raker.partial_fit(obs)\n",
    "    \n",
    "    # Record progress every 50 observations\n",
    "    if (i + 1) % 50 == 0:\n",
    "        time_points.append(i + 1)\n",
    "        margin_history.append(raker.margins.copy())\n",
    "        loss_history.append(raker.loss)\n",
    "        ess_history.append(raker.effective_sample_size)\n",
    "\n",
    "print(\"âœ… Streaming simulation complete!\")\n",
    "print(f\"ğŸ“Š Tracked {len(time_points)} checkpoints\")\n",
    "print(f\"ğŸ¯ Final ESS: {raker.effective_sample_size:.1f} / {n_obs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the streaming results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('ğŸŒŠ Streaming OnlineRake: Adapting to Time-Varying Bias', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Margin evolution over time\n",
    "features = list(streaming_targets.feature_names)\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, len(features)))\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    feature_margins = [margins[feature] for margins in margin_history]\n",
    "    axes[0,0].plot(time_points, feature_margins, '-o', \n",
    "                  label=f'{feature}', color=colors[i], markersize=4)\n",
    "    # Add target line\n",
    "    axes[0,0].axhline(y=streaming_targets[feature], color=colors[i], \n",
    "                     linestyle='--', alpha=0.7, linewidth=2)\n",
    "\n",
    "axes[0,0].set_xlabel('Observations')\n",
    "axes[0,0].set_ylabel('Weighted Margin')\n",
    "axes[0,0].set_title('ğŸ“ˆ Margin Evolution Over Time')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Loss convergence\n",
    "axes[0,1].plot(time_points, loss_history, '-o', color='red', markersize=4)\n",
    "axes[0,1].set_xlabel('Observations')\n",
    "axes[0,1].set_ylabel('Loss')\n",
    "axes[0,1].set_title('ğŸ“‰ Loss Over Time')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].set_yscale('log')\n",
    "\n",
    "# 3. Effective Sample Size evolution\n",
    "axes[1,0].plot(time_points, ess_history, '-o', color='blue', markersize=4)\n",
    "axes[1,0].set_xlabel('Observations')\n",
    "axes[1,0].set_ylabel('Effective Sample Size')\n",
    "axes[1,0].set_title('âš¡ ESS Evolution')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Final comparison\n",
    "final_margins = raker.margins\n",
    "target_vals = [streaming_targets[f] for f in features]\n",
    "final_vals = [final_margins[f] for f in features]\n",
    "errors = [abs(final_vals[i] - target_vals[i]) for i in range(len(features))]\n",
    "\n",
    "x = np.arange(len(features))\n",
    "width = 0.35\n",
    "\n",
    "axes[1,1].bar(x - width/2, target_vals, width, label='ğŸ¯ Target', alpha=0.8, color='gold')\n",
    "axes[1,1].bar(x + width/2, final_vals, width, label='âœ… Achieved', alpha=0.8, color='green')\n",
    "\n",
    "# Add error annotations\n",
    "for i, error in enumerate(errors):\n",
    "    axes[1,1].text(i, max(target_vals[i], final_vals[i]) + 0.02, \n",
    "                  f'Î”={error:.3f}', ha='center', fontsize=9, color='red')\n",
    "\n",
    "axes[1,1].set_xlabel('Features')\n",
    "axes[1,1].set_ylabel('Proportion')\n",
    "axes[1,1].set_title('ğŸ¯ Final Results vs Targets')\n",
    "axes[1,1].set_xticks(x)\n",
    "axes[1,1].set_xticklabels(features, rotation=45)\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ¨ Streaming visualization complete!\")\n",
    "print(\"ğŸŒŸ OnlineRake successfully adapted to changing patterns! âœ¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed final results\n",
    "print(\"\\nğŸ“‹ STREAMING RESULTS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "for feature in features:\n",
    "    target = streaming_targets[feature]\n",
    "    final = final_margins[feature]\n",
    "    error = abs(final - target)\n",
    "    improvement = (1 - error/abs(target - 0.5)) * 100 if abs(target - 0.5) > 0 else 100\n",
    "    print(f\"{feature:<12}: {final:.3f} (target: {target:.3f}, error: {error:.3f})\")\n",
    "\n",
    "avg_error = np.mean([abs(final_margins[f] - streaming_targets[f]) for f in features])\n",
    "print(f\"\\nğŸ“Š Average absolute error: {avg_error:.4f}\")\n",
    "print(f\"ğŸ¯ Final ESS: {raker.effective_sample_size:.1f} / {n_obs}\")\n",
    "print(f\"ğŸ“‰ Final loss: {raker.loss:.6f}\")\n",
    "\n",
    "if avg_error < 0.02:\n",
    "    print(\"\\nğŸ† EXCELLENT! Very low error achieved! ğŸ‰\")\n",
    "elif avg_error < 0.05:\n",
    "    print(\"\\nâœ… GOOD! Acceptable error level achieved! ğŸ‘\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ MODERATE: Consider tuning parameters for better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ Summary: OnlineRake Success!\n",
    "\n",
    "**Congratulations!** ğŸŠ You've successfully used OnlineRake to:\n",
    "\n",
    "âœ… **Correct feature bias** in real-time survey data  \n",
    "âœ… **Handle time-varying patterns** in streaming data  \n",
    "âœ… **Achieve target margins** with quantifiable accuracy  \n",
    "âœ… **Monitor performance** with comprehensive diagnostics  \n",
    "\n",
    "### ğŸ”‘ Key Takeaways:\n",
    "\n",
    "1. **SGD Raking** is fast and effective for most scenarios\n",
    "2. **MWU Raking** maintains positive weights through multiplicative updates  \n",
    "3. **Learning rates** can be tuned for convergence speed vs stability\n",
    "4. **Real-time monitoring** helps detect issues early\n",
    "5. **Visual validation** makes results immediately obvious\n",
    "\n",
    "### ğŸš€ Next Steps:\n",
    "\n",
    "- Try the **Performance Comparison** notebook for SGD vs MWU analysis\n",
    "- Explore **Advanced Diagnostics** for convergence monitoring\n",
    "- Check out the **API Reference** for all available options\n",
    "\n",
    "**Happy raking!** ğŸ¯âœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸŠ Thank you for using OnlineRake!\")\n",
    "print(\"ğŸ“š Check out the documentation for more examples and advanced features\")\n",
    "print(\"ğŸ› Found a bug or have a feature request? Please let us know!\")\n",
    "print(\"â­ If you found this useful, consider starring the repository! â­\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}